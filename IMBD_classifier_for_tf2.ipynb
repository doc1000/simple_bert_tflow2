{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this Towards Data Science [article](https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)\n",
    "\n",
    "This is a better [example](https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1#scrollTo=sm3lGfQb-1J8).  The above was hacked together from this one and is missing the vocab file for tokenization.\n",
    "\n",
    "This uses TensorFlow 2.0+ and TensorFlow Hub 0.7+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "#from bert import tokenization\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "# Keras is the new high level API for Tensorflow\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-for-tf2\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128 # your choice here\n",
    "\n",
    "#is_predicting = False\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The BERT layer requires 3 input sequence:\n",
    "\n",
    "- Token ids: for every token in the sentence.  We restore it from the BERT vocab dictionary\n",
    "- Mask ids: for every toekn to mask out tokens used only the sequence padding (so every sequence ahs the same length).\n",
    "- Segment ids: 0 for one-sentence sequence, 1 if there are two sentences in teh sequence and it is the second one (see the original paper or the corresponding part o the BERT on GitHub for more details: `convert_single_example` in the [`run_classifier.py`](https://github.com/google-research/bert/blob/master/run_classifier.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than the max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from the director in a DataFrame\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# merge positive and negative examples, add a polarity column and shuffle\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "    fname = 'aclImdb.tar.gz',\n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    extract=True)\n",
    "    \n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                        \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                       \"aclImdb\", \"test\"))\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = False\n",
    "if download:\n",
    "    train, test = download_and_load_datasets()\n",
    "    train.to_csv('\\saved_data\\train.csv')\n",
    "    test.to_csv('\\saved_data\\test.csv')\n",
    "else:\n",
    "    train = pd.read_csv('\\saved_data\\train.csv')\n",
    "    test = pd.read_csv('\\saved_data\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(1024)\n",
    "test = test.sample(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train['sentence'],train['polarity'].values\n",
    "x_test, y_test = test['sentence'],test['polarity'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "The goal of this model is to use the pre-trained BERT to gernate teh embeddoing vectors.  Tehrefore, we need only the required inputfs for the BERT layer and themodel has only the BERT layer as a hidden layer.  Of course, inside BERT, there is more cpomplex architecture.\n",
    "\n",
    "The `hub.KerasLayer` function oimports the pre-trained model as a keras layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, num_labels=32, hidden_size=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.output_weights = tf.Variable(\n",
    "            initial_value=tf.random.truncated_normal([num_labels, hidden_size],stddev=0.02),\n",
    "            dtype='float32', trainable=True)\n",
    "        self.bias_weights = tf.Variable(tf.zeros([num_labels]),\n",
    "                       dtype='float32', trainable=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output_layer = tf.nn.dropout(inputs, rate=0.1)\n",
    "\n",
    "        logits = tf.matmul(output_layer, self.output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, self.bias_weights)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        return log_probs\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_weights': self.output_weights,\n",
    "            'bias_weights': self.bias_weights,\n",
    "            \n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                      name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"segment_ids\")\n",
    "get_bert = False\n",
    "if get_bert:\n",
    "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "        trainable=False)\n",
    "    bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids],\n",
    "              outputs=[pooled_output, sequence_output])\n",
    "    bert_model.save('bert_model.h5')\n",
    "else:\n",
    "    bert_model = tf.keras.models.load_model('bert_model.h5')\n",
    "    l1, l2, l3, bert_layer = bert_model.layers\n",
    "                            \n",
    "                            \n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "output_layer = pooled_output #we're only using the pooled output for classification\n",
    "\n",
    "hidden_size = output_layer.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create our own layer to tune\\noutput_weights = tf.Variable(\\n  tf.random.truncated_normal([num_labels, hidden_size],stddev=0.02),\\n  name=\"output_weights\")\\n\\noutput_bias = tf.Variable(tf.zeros([num_labels]),\\n                       name=\"output_bias\")\\n\\n    #Dropout helps prevent overfitting\\noutput_layer = tf.nn.dropout(output_layer, rate=0.1)\\n\\nlogits = tf.matmul(output_layer, output_weights, transpose_b=True)\\nlogits = tf.nn.bias_add(logits, output_bias)\\nlog_probs = tf.nn.log_softmax(logits, axis=-1)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create our own layer to tune\n",
    "output_weights = tf.Variable(\n",
    "  tf.random.truncated_normal([num_labels, hidden_size],stddev=0.02),\n",
    "  name=\"output_weights\")\n",
    "\n",
    "output_bias = tf.Variable(tf.zeros([num_labels]),\n",
    "                       name=\"output_bias\")\n",
    "\n",
    "    #Dropout helps prevent overfitting\n",
    "output_layer = tf.nn.dropout(output_layer, rate=0.1)\n",
    "\n",
    "logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "logits = tf.nn.bias_add(logits, output_bias)\n",
    "log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\"\"\"\n",
    "linear_layer = Linear(num_labels,hidden_size)\n",
    "log_probs = linear_layer(output_layer)\n",
    "\n",
    "#log_probs = layers.Dense(2, activation='softmax')(output_layer)\n",
    "\n",
    "#predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "# If we're predicting, we want predicted labels and the probabiltiies.\n",
    "#if is_predicting:\n",
    "#    return (predicted_labels, log_probs)\n",
    "\n",
    "# If we're train/eval, compute loss between predicted and actual label\n",
    "\n",
    "#per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "#loss = tf.reduce_mean(per_example_loss)\n",
    "    \n",
    "    \n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], \n",
    "             outputs=[log_probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tokenizer\n",
    "Initialize a FullTokenizer by loading a `vocab_file` from the bert model layer and `do_lower_case`.  for some reason, this step is missing the the hacked together examples (which I based this off of.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "With these steps, we can generate BERT contextualized embedding vectors for our sentences!  Dont' forget to add [CLS] and [SEP] separator tokens to keep the original format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = FullTokenizer\n",
    "def convert_to_features(sentences):\n",
    "\n",
    "    \"\"\"s_list = []\n",
    "    s_list.append(\"This is a lame sentence\")\n",
    "    s_list.append(\"This is a nice sentence.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    input_segments = []\n",
    "\n",
    "    for s in sentences:\n",
    "        #s = \"This is a nice sentence.\"\n",
    "\n",
    "        stokens = tokenizer.tokenize(s)\n",
    "        stokens = stokens[:max_seq_length-2]\n",
    "        stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "        input_ids.append(get_ids(stokens, tokenizer, max_seq_length))\n",
    "        input_masks.append(get_masks(stokens, max_seq_length))\n",
    "        input_segments.append(get_segments(stokens, max_seq_length))\n",
    "\n",
    "    return np.array(input_ids), np.array(input_masks), np.array(input_segments)\n",
    "#pool_embs, all_embs = model.predict([[input_ids], [input_masks], [input_segments]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids, input_mask, segment_ids = convert_to_features(x_train)\n",
    "test_input_word_ids, test_input_mask, test_segment_ids = convert_to_features(x_test)\n",
    "\n",
    "# Convert labels into one-hot encoding\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=num_labels, dtype=tf.float32)\n",
    "y_test_one_hot = tf.one_hot(y_test, depth=num_labels, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples\n",
      "Epoch 1/3\n",
      "1024/1024 - 252s - loss: 0.6931 - accuracy: 0.4941\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "model.fit([input_word_ids, input_mask, segment_ids], y_train_one_hot,\n",
    "           epochs=1, batch_size=16, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "With these steps, we can generate BERT contextualized embedding vectors for our sentences!  Dont' forget to add [CLS] and [SEP] separator tokens to keep the original format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_predicted_labels, p_log_probs = model.predict([input_ids, input_masks, input_segments])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
